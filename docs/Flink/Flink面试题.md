---
custom_edit_url: null
---
### 简单介绍一下 Flink

Flink 是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。并且 Flink 提供了数据分布、容错机制以及资源管理等核心功能。Flink提供了诸多高抽象层的API以便用户编写分布式任务：

- **DataSet API**， 对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用Flink提供的各种操作符对分布式数据集进行处理，支持Java、Scala和Python。

- **DataStream API**，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持Java和Scala。

- **Table API**，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类SQL的DSL对关系表进行各种查询操作，支持Java和Scala。

![flink.png](..%2F..%2Fstatic%2Fimg%2Fflink.png)

  此外，Flink 还针对特定的应用领域提供了领域库，例如： Flink ML，Flink 的机器学习库，提供了机器学习Pipelines API并实现了多种机器学习算法。 Gelly，Flink 的图计算库，提供了图计算的相关API及多种图计算算法实现。

### Flink的主要特点是什么

Flink的主要特点包括：

- **流式处理和批处理一体化**：Flink既支持流式处理，也支持批处理，可以无缝地在流处理和批处理之间切换。
- **事件驱动的处理模型**：Flink使用事件时间和处理时间的概念，支持基于事件的处理和窗口操作，适用于实时数据处理和分析。
- **高性能和低延迟**：Flink的优化引擎可以实现高吞吐量和低延迟的数据处理，适用于需要快速响应的应用场景。
- **容错性和可靠性**：Flink具有容错机制，可以在节点故障时保证数据处理的正确性和一致性。
- **灵活的编程模型**：Flink支持多种编程模型，包括基于流的API（DataStream API）和基于批的API（DataSet API），并提供了多种编程语言接口

### 讲一下Flink的运行架构，Flink集群有哪些角色

Flink程序在运行时主要有TaskManager，JobManager，Client三种角色。

当 Flink 集群启动后，⾸先会启动⼀个 JobManger 和⼀个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执⾏，然后 TaskManager 将⼼跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进⾏数据的传输。上述三者均为独⽴的 JVM 进程

- **JobManager**：扮演着集群中的管理者Master的角色，它是整个集群的协调者，负责接收Flink Job，协调检查点，Failover 故障恢复等，同时管理Flink集群中从节点TaskManager。
- **TaskManager**：是实际负责执行计算的Worker，在其上执行Flink Job的一组Task，每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。
- **Client**：是Flink程序提交的客户端，当用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager。

### Flink的集群部署模式有哪些

Flink的集群部署模式包括：

- **单机模式**：在单个机器上运行Flink集群，适用于开发和测试环境。
- **本地模式**：在本地的多个线程上模拟Flink集群，适用于开发和调试任务。
- **分离式部署**：将JobManager和TaskManager分别部署在不同的机器上，适用于生产环境和大规模任务的执行。
- **嵌入式模式**：将Flink集成到现有的应用程序中，作为库来使用，适用于需要将流处理能力集成到其他应用中的场景。

### 你们之前Flink集群规模有多大

集群规模通常与公司的业务需求、数据量、计算资源等因素密切相关。

在实际应用中，Flink 集群的规模可能会从几台到几十台甚至上百台节点不等。集群规模的大小取决于业务需求和数据处理能力。大型互联网公司可能需要处理更多的数据和请求，因此他们的 Flink 集群规模可能会更大。

在部署方式上，大部分公司采用 YARN 模式进行部署。YARN 提供了一种分布式资源管理的方式，可以更好地调度和管理集群中的资源。根据公司的需求和资源配置，YARN 可以选择不同的部署模式，如单 job 模式或批量模式。这些部署模式的选择会影响到集群的规模和性能。

总之，Flink 集群规模的大小会根据公司的实际需求和资源配置而有所不同，需要根据具体情况进行评估和优化。

### 说说Flink集群优化

Flink 集群优化是提高 Flink 集群性能的关键步骤。

以下是一些 Flink 集群优化的建议：

1. **taskmanager.heap.mb 调优**：taskmanager.heap.mb 是 Flink 任务管理器堆内存的大小，默认为 1024MB。如果需要更高的内存，可以将其调整为 2048MB 或更高。这可以确保任务管理器有足够的内存来处理数据和执行任务。
2. **调整执行任务的并行度**：Flink 任务的并行度可以通过任务属性进行调整。增加并行度可以提高任务的执行速度，但也会增加内存和 CPU 的使用量。因此，需要根据具体情况调整任务的并行度。
3. **优化任务调度**：Flink 任务调度可以通过多种方式进行优化。例如，可以调整 taskmanager 的数量和分配策略，以确保任务在不同的 taskmanager 上均匀分配。还可以调整任务的优先级和资源要求，以确保任务能够优先获得所需的资源。
4. **优化网络配置**：Flink 集群的网络配置也对性能有很大的影响。例如，可以调整 taskmanager 之间的连接方式，以确保任务数据能够快速传输。还可以调整网络带宽和延迟，以确保任务能够在规定时间内完成。
5. **优化状态管理**：Flink 任务的状态管理也是一个重要的优化方面。例如，可以使用 Flink 的状态备份和恢复功能，以确保任务状态能够在集群中的不同节点之间同步。还可以调整状态的持久化方式和位置，以确保状态数据不会丢失。
6. **使用 Flink 的高级优化功能**：Flink 还提供了许多高级优化功能，例如代码生成、优化器和迭代算子等。这些功能可以显著提高 Flink 集群的性能，但需要根据具体情况进行调整和使用。

总结起来，Flink 集群优化需要综合考虑多个方面，包括内存管理、任务调度、网络配置、状态管理和高级优化功能等。通过调整这些参数和配置，可以显著提高 Flink 集群的性能和效率。

### 公司怎么提交的实时任务，有多少Job Manager

1）我们使用yarn session模式提交任务；另一种方式是每次提交都会创建一个新的Flink 集群，为每一个job提供资源，任务之间互相独立，互不影响，方便管理。任务执行完成之后创建的集群也会消失。线上命令脚本如下：

```bash
bin/yarn-session.sh -n 7 -s 8 -jm 3072 -tm 32768 -qu root.*.* -nm *-* -d
```

其中申请7个 taskManager，每个 8 核，每个 taskmanager 有 32768M 内存。

2）集群默认只有一个 Job Manager。但为了防止单点故障，我们配置了高可用。对于standlone模式，我们公司一般配置一个主 Job Manager，两个备用 Job Manager，然后结合 ZooKeeper 的使用，来达到高可用；对于yarn模式，yarn在Job Mananger故障会自动进行重启，所以只需要一个，我们配置的最大重启次数是10次。

### Flink的并行度了解吗

Flink 程序由多个任务（Source、Transformation、Sink）组成。任务被分成多个并行实例来执行，每个并行实例处理任务的输入数据的子集。任务的并行实例的数量称之为并行度。

我们在实际生产环境中可以从四个不同层面设置并行度：

- 操作算子层面 (Operator Level)：算子.setParallelism(3),实际算子时设置
- 执行环境层面 (Execution Environment Level)：构建Flink环境时getExecutionEnvironment.setParallelism(1)设置
- 客户端层面 (Client Level)：提交flink run -p的时候设置
- 系统层面 (System Level)：flink客户端的配置yml文件中设置

需要注意的**优先级：算子层面>环境层面>客户端层面>系统层面**（实际业务中通常设置和kafka分区数一样或者kafka分区倍数的并行度）。

Flink 可以设置好几个 level 的 parallelism，其中包括 Operator Level、ExecutionEnvironment Level、Client Level、System Level

在 flink-conf.yaml 中通过parallelism.default 配置项给所有 execution environments 指定系统级的默认parallelism；

在 ExecutionEnvironment 里头可以通过 setParallelism 来给 operators、data sources、data sinks 设置默认的 parallelism；

如果 operators、data sources、datasinks 自己有设置 parallelism 则会覆盖 ExecutionEnvironment 设置的 parallelism

### Flink的Checkpoint存在哪里

Flink 的 Checkpoint 是 Flink 的核心组件之一，它用于记录应用程序在特定时刻的状态，以便在应用程序失败时进行恢复。Checkpoint 通常存储在 Flink 的存储系统中，可以是内存、文件系统或 RocksDB。

**1. 内存**

Flink 的内存状态是存储在 Java 内存中的。当应用程序运行时，Flink 会将状态数据存储在内存中，并定期将这些状态数据持久化到外部存储系统中。如果应用程序在运行时出现故障，Flink 可以从内存状态中恢复应用程序的状态。

**2. 文件系统**

Flink 也可以将状态数据存储在文件系统中。当应用程序运行时，Flink 会将状态数据写入分布式文件系统，如 HDFS 或 NFS。如果应用程序在运行时出现故障，Flink 可以从文件系统中恢复应用程序的状态。

**3. RocksDB**

Flink 还可以将状态数据存储在 RocksDB 中。RocksDB 是一种高性能、高可靠性的键值存储数据库，它支持高效的数据压缩和快速查找。当应用程序运行时，Flink 会将状态数据写入 RocksDB 数据库，并定期将这些状态数据持久化到外部存储系统中。如果应用程序在运行时出现故障，Flink 可以从 RocksDB 中恢复应用程序的状态。

总之，Flink 的 Checkpoint 可以存储在内存、文件系统或 RocksDB 中，具体存储位置由用户配置决定。Flink 提供了一些 API 来管理 Checkpoint，如 checkpointCoordinator.checkpoint() 方法和 checkpointCoordinator.restoreFromCheckpoint() 方法。使用这些 API，用户可以手动触发 Checkpoint，也可以在应用程序失败时自动恢复状态

### Flink的checkpoint机制对比spark有什么不同和优势

Flink 和 Spark 都是主流的大数据处理框架，它们都支持 Checkpoint 机制以保证实时数据的可靠性和容错性。然而，Flink 和 Spark 的 Checkpoint 机制在实现方式和功能上有一些不同之处。

**1. 实现方式**

Flink 的 Checkpoint 机制采用了轻量级的分布式快照技术，实现了每个算子的快照以及流动中的数据的快照。这种快照技术可以快速地保存和恢复状态数据，从而减少了故障恢复的时间。而 Spark 的 Checkpoint 机制主要是针对 Driver 的故障恢复做了数据和元数据的 Checkpoint，没有实现算子的快照。

**2. 故障恢复**

Flink 的 Checkpoint 机制可以支持任意节点的故障恢复，包括算子和 Driver。当一个节点出现故障时，Flink 会自动切换到其他可用节点，并从最近的 Checkpoint 开始恢复状态数据。而 Spark 的 Checkpoint 机制只能恢复 Driver 的故障，对于算子的故障则需要重新启动整个应用程序。

**3. 数据一致性**

Flink 的 Checkpoint 机制可以保证数据一致性，即同一个 Checkpoint 下的所有算子都处于同一个状态。这是因为 Flink 使用了分布式快照技术，确保每个算子都保存了相同的状态数据。而 Spark 的 Checkpoint 机制并不能保证数据一致性，因为在 Spark 中，每个算子都可能保存了不同的状态数据。

**4. 性能影响**

Flink 的 Checkpoint 机制采用了轻量级的分布式快照技术，因此其性能影响相对较小。Spark 的 Checkpoint 机制需要将整个应用程序的状态数据都保存到外部存储系统中，因此其性能影响相对较大。

总的来说，Flink 的 Checkpoint 机制相对于 Spark 的 Checkpoint 机制更为复杂和强大，可以支持任意节点的故障恢复，并保证数据一致性。此外，Flink 的 Checkpoint 机制采用了轻量级的分布式快照技术，因此其性能影响相对较小。这些优势使得 Flink 在实时数据处理方面具有更好的可靠性和容错性

### Flink的流式处理如何处理延迟

Flink的流式处理可以通过以下方式处理延迟：

事件时间处理：Flink支持事件时间处理，可以处理乱序事件，根据事件时间对数据进行排序和处理，从而解决延迟问题。

窗口操作：Flink的窗口操作可以根据事件时间或处理时间对数据流进行划分和处理，可以根据需要设定窗口大小和滑动间隔来控制延迟的处理。

### Flink 中对窗口的支持包括哪几种？说说他们的使用场景

Flink支持两种划分窗口的方式，按照time和count，session也是一种时间。

- **Tumbling Time Window（滚动时间窗口）**：当达到一定时间后的，进行滑动，可以联想到以前用的诺基亚的滑盖手机，这个其实就是微批。用于处理实时数据流中的时间序列数据，如股票价格走势、实时监测流量等。假如我们需要统计每一分钟中用户购买的商品的总数，需要将用户的行为事件按每一分钟进 行切分，这种切分被成为翻滚时间窗口（Tumbling Time Window）。翻滚窗口能将数据流切分成 不重叠的窗口，每一个事件只能属于一个窗口。

- **Sliding Time Window（滑动时间窗口）**：当达到一定时间后，进行翻滚，可以有重叠。用于处理时间序列数据的近期分析，如近 5 分钟用户购买商品总数等。我们可以每 30 秒计算一次最近一分钟用户购买的商品总数。这种窗口我们称为滑动时间窗 口（Sliding Time Window）。在滑窗中，一个元素可以对应多个窗口。

- **Tumbling Count Window（滚动计数窗口）**：当达到一定条数的时候执行计算，无折叠。用于处理计数型数据，如统计网站访问量、分析用户购买行为等。当我们想要每 100 个用户购买行为事件统计购买总数，那么每当窗口中填满 100 个元素了， 就会对窗口进行计算，这种窗口我们称之为翻滚计数窗口（Tumbling Count Window），上图所 示窗口大小为 3 个。

- **Sliding Count Window（滑动计数窗口）**：当达到一定数量后进行滑动。用于处理计数型数据的实时分析，如实时监测广告点击率、实时统计投票数等。

- **Session Window（会话窗口）**：窗口数据没有固定的大小，根据用户传入的参数进行划分，窗口数据无叠加。用于处理用户交互事件流中的数据，如计算每个用户在活跃期间总共购买的商品数量等。类似于当用户退出的时候，计算这个用户之前的动作。在这种用户交互事件流中，我们首先想到的是将事件聚合到会话窗口中（一段用户持续活跃 的周期），由非活跃的间隙分隔开。如上图所示，就是需要计算每个用户在活跃期间总共购买的 商品数量，如果用户 30 秒没有活动则视为会话断开（假设 raw data stream 是单个用户的购买 行为流）。一般而言，window 是在无限的流上定义了一个有限的元素集合。这个集合可以是基于时间的，元素个数的，时间和个数结合的，会话间隙的，或者是自定义的。Flink 的 DataStream API 提供了简洁的算子来满足常用的窗口操作，同时提供了通用的窗口机制来允许用户自己定义 窗口分配逻辑。


### 说说Flink 的容错机制，Flink是如何做到容错的

Flink 是一个分布式流处理框架，它实现了容错机制以确保在节点故障时，数据不会丢失并且可以进行故障恢复。Flink 的容错机制主要依靠两个强大的机制：Checkpoint 和 State。

- **Checkpoint**：是一种快照机制，它用于定期备份 Flink 程序中的状态，并将其存储在外部存储系统中。当节点发生故障时，Flink 可以使用 Checkpoint 来恢复程序的状态，并从故障点继续处理数据流。Checkpoint 的备份可以是全量的，也可以是增量的，这取决于 Checkpoint 的触发条件和备份策略。Flink 还支持 Exactly-Once 语义，这意味着在故障恢复时，Flink 可以确保每个事件都被处理了一次且仅一次。
- **State**：是 Flink 中的另一种重要机制，它用于存储计算过程中的中间状态。State 可以分为两种类型：Operator State 和 Keyed State。Operator State 是一种基于算子的状态，它存储在算子内部，并随着算子的执行而更新。Keyed State 是一种基于键的状态，它存储在 Stateful Function 内部，并使用键来标识状态的数据。Keyed State 可以具有过期时间（TTL），这使得 Flink 可以在状态过期时自动清理过期的状态数据。

在 Flink 中，Checkpoint 和 State 是相互依存的。Checkpoint 用于备份 State，并确保在节点故障时，可以恢复程序的状态。而 State 则用于存储计算过程中的中间状态，并支持 Exactly-Once 语义。Flink 通过这两个机制的结合，实现了强大的容错和故障恢复能力，使得 Flink 在分布式流处理中具有高度的可靠性和可用性。

### Flink程序在面对数据高峰期时如何处理

当 Flink 程序面对数据高峰期时，一种常用的方法是使用大容量的 Kafka 作为数据源，将数据先放到消息队列中，然后再使用 Flink 进行消费。这种方法可以有效地削峰平谷，减缓数据流量对 Flink 程序的影响，从而提高程序的稳定性和可靠性。

不过，使用 Kafka 作为数据源会影响一点实时性。因为 Kafka 是一个异步的消息队列，数据在队列中需要等待消费者消费，所以会存在一定的延迟。为了解决这个问题，可以采用以下方法：

1. 调整 Kafka 的参数，如增大 Kafka 的缓存大小、增加 Kafka 的并发消费者数量等，以提高 Kafka 的吞吐量和处理能力。
2. 优化 Flink 程序的配置，如增大 Flink 的并行度、调整 Flink 的内存配置等，以提高 Flink 的处理能力和吞吐量。
3. 采用 Flink 中的 Stateful Functions 或 Checkpointing 功能，以保持数据的一致性和可靠性。Stateful Functions 可以让 Flink 程序对数据的处理具有状态感知能力，从而更好地处理数据流中的事件。而 Checkpointing 功能可以让 Flink 程序在处理数据时，定期将中间状态持久化到外部存储系统中，以便在程序失败时进行恢复。

综上所述，使用 Kafka 作为数据源可以有效地处理数据高峰期，但需要注意 Kafka 和 Flink 的配置优化，以及数据处理的实时性和一致性问题

### Flink分布式快照的原理是什么

Flink的容错机制的核心部分是制作分布式数据流和操作算子状态的一致性快照。 这些快照充当一致性checkpoint，系统可以在发生故障时回滚。 Flink用于制作这些快照的机制在“分布式数据流的轻量级异步快照”中进行了描述。 它受到分布式快照的标准Chandy-Lamport算法的启发，专门针对Flink的执行模型而定制。

barriers在数据流源处被注入并行数据流中。快照n的barriers被插入的位置（我们称之为Sn）是快照所包含的数据在数据源中最大位置。

例如，在Apache Kafka中，此位置将是分区中最后一条记录的偏移量。 将该位置Sn报告给checkpoint协调器（Flink的JobManager）。

然后barriers向下游流动。当一个中间操作算子从其所有输入流中收到快照n的barriers时，它会为快照n发出barriers进入其所有输出流中。

一旦sink操作算子（流式DAG的末端）从其所有输入流接收到barriers n，它就向checkpoint协调器确认快照n完成。

在所有sink确认快照后，意味快照着已完成。一旦完成快照n，job将永远不再向数据源请求Sn之前的记录，因为此时这些记录（及其后续记录）将已经通过整个数据流拓扑，也即是已经被处理结束。

### Flink跟Spark Streaming的区别

这个问题是一个非常宏观的问题，因为两个框架的不同点非常之多。但是在面试时有非常重要的一点一定要回答出来：Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批（Micro-Batch）的模型。

下面我们就分几个方面介绍两个框架的主要区别：

1. 架构模型Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、Executor，Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。
2. 任务调度Spark Streaming 连续不断的生成微小的数据批次，构建有向无环图DAG，Spark Streaming 会依次创建 DStreamGraph、JobGenerator、JobScheduler。Flink 根据用户提交的代码生成 StreamGraph，经过优化生成 JobGraph，然后提交给 JobManager进行处理，JobManager 会根据 JobGraph 生成 ExecutionGraph，ExecutionGraph 是 Flink 调度最核心的数据结构，JobManager 根据 ExecutionGraph 对 Job 进行调度。
3. 时间机制Spark Streaming 支持的时间机制有限，只支持处理时间。 Flink 支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时间。同时也支持 watermark 机制来处理滞后数据。
4. 容错机制对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰好一次处理语义。Flink 则使用两阶段提交协议来解决这个问题

### 说说Flink的几种时间语义

Flink 支持三种时间语义：Event Time、Ingestion Time 和 Processing Time。

**1. Event Time（事件时间）**

Event Time 是事件创建的时间，它通常由事件中的时间戳描述。通常由事件生成器或者传感器生成。在 Flink 中，事件时间可以通过 water-mark 或者定时器来处理。例如，在采集日志数据时，每一条日志都会记录自己的生成时间，Flink 通过时间戳分配器访问事件时间戳。Event Time 是事件产生的时间，与数据处理的时间无关，因此它可以反映事件产生的实时性，但是对于数据处理的延迟和异步性无法体现。

**2. Ingestion Time（注入时间）**

Ingestion Time 是数据进入 Flink 的时间。它是指数据被 Flink 算子处理的时间，与事件创建的时间无关。Ingestion Time 能够反映数据处理的延迟和异步性，但是无法反映事件产生的实时性。

**3. Processing Time（处理时间）**

Processing Time 是每一个执行基于时间操作的算子的本地系统时间，与机器相关。它是指算子处理数据的时间，与事件创建的时间和数据进入 Flink 的时间无关。Processing Time 是默认的时间属性，除非明确指定时间语义为 Event Time 或 Ingestion Time。

在实际应用中，选择合适的时间语义可以影响 Flink 处理的数据流的正确性和效率。

例如，如果需要处理实时数据流，那么选择 Event Time 更为合适；

如果需要处理延迟数据流，那么选择 Ingestion Time 更为合适；

如果需要处理离线数据集，那么选择 Processing Time 更为合适。

同时，Flink 也提供了 WaterMark 机制来处理延迟数据和异步数据，以保证数据处理的正确性和可靠性

### 说说Flink中的Watermark机制

Flink 中的 Watermark 机制是一种衡量 Event Time 进展的机制，可以用于处理乱序事件。在数据流处理过程中，由于网络延迟、背压等多种因素的影响，数据可能会乱序到达。为了正确处理这些乱序事件，Flink 引入了 Watermark 机制，结合窗口 (Window) 来实现。

Watermark 是一个时间戳，用于表示事件时间小于等于该时间戳的数据都已经到达。在 Flink 中，每个 Operator 都会维护一个当前的 Watermark，当一个事件到达时，如果它的时间戳小于等于当前 Watermark，那么该事件就会被认为是到达了，会被放入窗口中进行处理。窗口的执行是由 Watermark 触发的，当 Watermark 达到窗口的结束时间时，窗口就会触发并执行其中的计算逻辑。

为了实现窗口的正确处理，Flink 还引入了事件时间 (Event Time) 概念，每个事件都会携带一个时间戳，表示该事件产生的时间。在数据流处理过程中，Flink 会根据事件时间戳的顺序来处理事件，这样可以保证事件的正确顺序。但是，由于网络延迟、背压等原因，事件可能会乱序到达，这就需要使用 Watermark 机制来处理这些乱序事件。

总结起来，Flink 中的 Watermark 机制是用于处理乱序事件的一种机制，它可以设定延迟触发，用于表示事件时间小于等于该时间戳的数据都已经到达。通过结合窗口机制，Watermark 机制可以实现对乱序事件的正确处理，保证数据流的正确性和完整性。

### Flink怎么做压力测试和监控

产生的数据流的速度如果过快，而下游的算子消费不过来的话，会产生背压。背压的监控可以使用Flink Web UI来可视化监控Metrics，一旦报警就能知道。一般情况下可能由于sink这个操作符没有优化好，做一下优化就可以了。

设置watermark的最大延迟时间这个参数，如果设置的过大，可能会造成内存的压力。可以设置最大延迟时间小一些，然后把迟到的元素发送到测输出流中，晚一点更新结果。

还有就是滑动窗口的长度如果过大，而滑动距离很短的话，Flink的性能也会下降的厉害。可以通过分片的方法，将每个元素只存入一个“重叠窗口”，这样就可以减少窗口处理中状态的写入。

### Flink是通过什么机制实现的背压机制

Flink在运行时主要由operators和streams两大构件组成。每个operator会消费中间状态的流，并在流上进行转换，然后生成新的流。对于Flink的网络机制一种形象的类比是，Flink使用了高效有界的分布式阻塞队列，就像Java通过的阻塞队列（BlockingQueue）一样。使用BlockingQueue的话，一个较慢的接受者会降低发送者的发送速率，因为一旦队列满了（有界队列）发送者会被阻塞。

在Flink中，这些分布式阻塞队列就是这些逻辑流，而队列容量通过缓冲池（LocalBufferPool）实现的。每个被生产和消费的流都会被分配一个缓冲池。缓冲池管理者一组缓冲（Buffer），缓冲在被消费后可以被回收循环利用。

### Flink是如何处理反压的？如何监控和发现

Flink 的反压（Backpressure）是指当一个 Operator 的输出速度比其下游 Operator 的输入速度慢时，下游 Operator 可能会积累一定数量的数据，导致处理速度变慢，甚至堵塞。为了解决这个问题，Flink 引入了反压机制，以便及时发现并解决数据处理速度不匹配的问题。

Flink 在处理反压问题时，并没有使用复杂的机制，而是采用了一种简单而高效的方法。Flink 在数据传输过程中使用了分布式阻塞队列，从而有效地解决了反压问题。

在 Flink 中，当一个算子的输出速度比下游算子快时，Flink 会使用分布式阻塞队列来缓存输出数据。这样可以避免上游算子过快地生成数据，导致下游算子无法及时处理，从而形成反压。当下游算子需要数据时，它会从队列中取出数据并进行处理。当队列中的数据达到一定阈值时，上游算子会收到通知，从而减缓数据生成速度。这样一来，Flink 就通过分布式阻塞队列实现了反压的缓解。

另外，Flink 通过每个 TaskManager 和 JobManager 之间的通信来实现反压的缓解。当下游处理任务时间太长时，Flink 会检测到这种情况，并认为这是一个反压信号。此时，Flink 会将这个反压信号传递给上游任务的管理器。

具体来说，Flink 的反压策略主要分为以下几个步骤：

1. **任务反压**：当下游任务的处理速度较慢时，Flink 会检测到这种情况，并认为这是一个反压信号。此时，Flink 会将这个反压信号传递给上游任务的管理器。
2. **调整数据生成速度**：当上游任务的管理器收到反压信号后，会根据反压信号的强度来调整数据生成速度。通常情况下，反压信号越强，上游任务生成的数据量就会减少，以减轻下游任务的负担。
3. **控制反压**：Flink 还会通过一些控制机制来避免过度反压。例如，当上游任务的数据生成速度过慢时，Flink 会限制反压的强度，以避免数据积压过多。此外，Flink 还会设置一个反压阈值，当反压信号超过这个阈值时，Flink 会认为任务已经处于一个不稳定的状态，并会采取相应的措施，如调整任务并行度、暂停任务等。
4. **恢复数据生成速度**：当下游任务的处理速度恢复到正常水平时，Flink 会检测到这个变化，并逐渐增加上游任务的数据生成速度，以恢复数据流。

可以根据下游任务的处理速度来动态调整上游任务的数据生成速度，以缓解数据积压问题。这种策略在实际应用中可以提高 Flink 任务的处理效率和稳定性。

Flink 的反压监控和发现主要通过以下方式进行：

1. **Flink Web UI**：Flink Web UI 是一个基于 Web 的用户界面，用于管理和监控 Flink 集群。在 Flink Web UI 中，用户可以查看作业的运行状态、任务管理信息以及反压状态。具体地，在“Jobs”页面中，用户可以查看每个作业的 Backpressure 状态，包括 OK、LOW 和 HIGH 三种状态。此外，在“Task Managers”页面中，用户还可以查看每个 TaskManager 的心跳信息和反压状态。
2. **Flink 命令行工具**：除了 Web UI 外，用户还可以使用 Flink 提供的命令行工具（如“flink”和“jobmanager”）来进行反压监控。例如，使用“jobmanager”命令，用户可以查看作业的详细信息，包括任务状态和反压状态。
3. **第三方监控工具**：除了 Flink 自带的监控工具外，还有一些第三方的 Flink 监控工具可以帮助用户监控反压状态。例如，Apache Kafka 提供了一个名为“Kafka Console Consumer”的工具，用于查看 Kafka 主题的消费情况。通过这个工具，用户可以了解数据生产的速度，从而判断是否存在反压问题。
4. **自定义监控与报警**：为了更加实时和准确地监控反压状态，用户可以编写自定义的监控与报警脚本。这些脚本可以定期获取 Flink 集群的状态信息，并根据预设的规则发送告警通知。例如，当发现某个 Operator 的反压状态为 HIGH 时，可以自动发送告警邮件给相关人员。

总之，Flink 通过 Web UI、命令行工具、第三方监控工具以及自定义监控与报警等多种方式，帮助用户实时监控和发现反压问题，从而确保数据处理的高效和稳定。

### Flink中的Window出现了数据倾斜，你有什么解决办法

Flink 中的窗口操作是一种基于时间窗口的数据处理方式，可以用于统计分析、监控、实时计算等应用场景。然而，当数据量过大或者数据发送速度不均匀时，可能会导致窗口中堆积的数据量相差过多，即出现了数据倾斜的情况。

数据倾斜会对 Flink 的性能产生负面影响，因为窗口计算需要对所有数据进行聚合操作，而数据倾斜会导致部分窗口的数据量过大，从而增加计算时间和资源消耗。为了解决数据倾斜问题，可以采用以下几种方法：

1. **在数据进入窗口之前做预聚合**：这种方法可以在数据进入窗口之前，先进行一定的聚合操作，使得每个窗口中的数据量相对均匀。具体的做法可以是在数据源处进行预聚合，或者在 Flink 中使用 DataStream API 中的窗口聚合函数（如 Tumbling Windows 和 Sliding Windows）进行预聚合。
2. **重新设计窗口聚合的 key**：在某些情况下，窗口聚合的 key 可能需要进行重新设计，以避免数据倾斜。例如，可以将 key 设计为数据发送的时间戳，而不是数据本身的某些属性。这样可以使得窗口中的数据量更加均匀，从而避免数据倾斜。
3. **调整窗口参数**：在某些情况下，可以通过调整窗口参数来避免数据倾斜。例如，可以增加窗口的大小或者增加窗口的滑动间隔，使得窗口中的数据量更加均匀。
4. **使用 Flink 的 Ttl 操作**：Flink 中的 Ttl 操作可以在数据到达窗口时，根据数据的时间戳进行淘汰操作，从而避免数据倾斜。具体的做法是设置一个 Ttl 时间，当数据到达窗口时，如果数据的时间戳已经超过了 Ttl 时间，则将该数据淘汰，从而避免数据倾斜。

综上所述，解决 Flink 中的窗口数据倾斜问题需要根据具体情况进行分析和处理。可以采用预聚合、重新设计窗口聚合的 key、调整窗口参数或者使用 Flink 的 Ttl 操作等方法来避免数据倾斜，从而提高 Flink 的性能和可靠性。

### 使用KeyBy算子时，某一个Key的数据量过大，导致数据倾斜，怎么处理

当使用 KeyBy 算子时，如果某个 Key 的数据量过大，会导致数据倾斜，影响计算效率。为了解决这个问题，可以考虑以下方法：

1. 将 Key 进行散列，将 Key 转换为 Key-随机数的形式，这样可以保证数据散列，对打散后的数据进行聚合统计。这时，我们会得到原始的 Key 加上随机数的统计结果。
2. 将散列的 Key 去除拼接的随机数，得到原始的 Key，然后进行二次 KeyBy 进行结果统计。这样可以保证数据倾斜不会影响最终的结果。

### Flink在使用聚合函数之后出现数据热点怎么解决

Flink 在使用聚合函数之后出现数据热点的问题，主要是由于某些聚合函数的计算量比较大，导致数据处理速度较慢，从而产生了数据积压和延迟。这种情况下，可以通过以下几种方法来解决数据热点问题：

1. **增加计算资源**：增加计算节点和内存资源，提高 Flink 集群的计算能力，从而加快数据处理速度，降低数据积压和延迟。
2. **调整聚合函数参数**：有些聚合函数的计算量比较大，可以考虑调整聚合函数的参数，减少计算量，从而提高数据处理速度。例如，可以调整窗口的大小或者滑动间隔等参数。
3. **使用批量处理**：将数据按照一定的时间间隔进行批量处理，降低实时处理的压力，从而减少数据积压和延迟。例如，可以使用 Flink 的 Batch 操作进行批量处理。
4. **采用数据重复消除策略**：在某些情况下，数据热点可能是由于某些数据的重复导致的。可以采用数据重复消除策略，例如使用 Flink 中的 Checkpointing 操作，从而避免重复数据导致的数据热点问题。
5. **调整数据源参数**：在某些情况下，数据源的参数设置可能导致数据热点问题。可以调整数据源的参数，例如发送数据的间隔时间、数据压缩等方式，从而降低数据热点问题。

综上所述，解决 Flink 中的数据热点问题需要根据具体情况进行分析和处理。可以采用增加计算资源、调整聚合函数参数、使用批量处理、采用数据重复消除策略或者调整数据源参数等方法来解决数据热点问题，从而提高 Flink 的性能和可靠性。

### Flink 任务延迟高，想解决这个问题，你会如何入手

如果 Flink 任务延迟高，需要从以下几个方面入手进行优化：

1. **资源调优**：首先检查 Flink 集群的资源使用情况，如果发现某些节点资源使用率过高，可以考虑增加节点数量或者调整节点的资源配置，如增加内存、CPU 等。此外，还可以调整任务管理器的资源分配策略，如优先使用空闲节点等。
2. **算子调优**：如果任务延迟高，可以考虑调整算子的参数，如窗口时长、并发数等。窗口时长越短，计算量越大，可能会导致延迟增加，因此需要根据具体情况进行调整。同时，可以考虑使用更高效的算子，如 Reducer 的并行度可以调整为 taskNumber 的因子等。
3. **数据优化**：数据优化是提高 Flink 任务性能的重要手段。可以考虑使用数据压缩、数据筛选、数据重复消除等技术，以减少数据量和计算量。同时，还可以考虑使用批量处理、Checkpointing 等技术，以优化数据处理流程。
4. **任务调度优化**：任务调度优化也是提高 Flink 任务性能的重要手段。可以考虑使用 Flink 自带的调度器，如 FairScheduler、DynamicTaskAllocation 等，这些调度器可以根据不同的策略分配任务和管理器。此外，还可以使用自定义的调度器，如基于优先级、基于资源使用情况等调度器，以优化任务调度。
5. **错误处理**：如果任务出现错误，可能会导致延迟增加。因此，需要设置正确的错误处理策略，如使用 try-catch 语句、设置错误处理延迟等，以避免错误导致的延迟增加。

综上所述，要解决 Flink 任务延迟高的问题，需要从资源调优、算子调优、数据优化、任务调度优化和错误处理等方面入手，以提高 Flink 任务的性能和可靠性。

### Flink是如何保证Exactly-once语义的

**如果下级存储支持事务**：

Flink可以通过实现两阶段提交和状态保存来实现端到端的一致性语义。

分为以下几个步骤：

1. 开始事务（beginTransaction）创建一个临时文件夹，来写把数据写入到这个文件夹里面
2. 预提交（preCommit）将内存中缓存的数据写入文件并关闭
3. 正式提交（commit）将之前写完的临时文件放入目标目录下。这代表着最终的数据会有一些延迟
4. 丢弃（abort）丢弃临时文件
5. 若失败发生在预提交成功后，正式提交前。可以根据状态来提交预提交的数据，也可删除预提交的数据。

**下级存储不支持事务**：

端到端的exactly-once对sink的要求比较高，具体的实现主要有幂等写入和事务性写入两种方式。幂等写入的场景依赖于业务逻辑，更常见的是用事务性写入。而事务性写入又有预写日志（WAL）和两阶段提交（2PC）两种方式。

如果外部系统不支持事务，那么可以使用预写日志的方式，把结果数据当成状态保存，然后在收到checkpoint完成的通知时，一次性写入sink系统

### 说说Flink的状态

在 Flink 中，状态是指在实时计算过程中，用于存储和处理数据的一种机制。状态可以分为两种基本类型：KeyedState 和 OperatorState。

- **KeyedState**：是基于键（Key）的状态，通常和 KeyedStream 的操作相关。KeyedState 包含两种基本的状态：ValueState 和 MapState。ValueState 用于存储单一值的状态，而 MapState 用于存储映射关系。在实际生产中，通常使用 KeyedState 中的 ValueState 和 MapState。
- **OperatorState**：是基于算子（Operator）的状态，通常和非 KeyedStream 的操作相关。OperatorState 可以存储算子的内部状态，例如窗口状态、累加器等。

KeyedState 和 OperatorState 都是 Flink 中的状态类型，它们在实时计算中起到了重要的作用。KeyedState 通常用于处理基于键的数据，例如对某个键进行计数、聚合等操作；而 OperatorState 通常用于处理非基于键的数据，例如对数据进行窗口操作、排序等操作。

在学习 Flink 中的状态时，需要了解状态的基本概念、分类、使用方式以及状态管理的相关概念。同时，需要掌握如何在程序中使用 KeyedState 和 OperatorState，以便在实时计算中处理数据。

### 说说Flink的状态储存机制

Flink 的状态储存是指在 Flink 程序运行过程中，用于存储和管理算子状态的数据结构和存储系统。Flink 提供了多种状态后端，以适应不同的应用场景和需求。这里将详细叙述 Flink 的状态储存，包括 1.13 版本之前的状态后端和 1.13 版本之后的状态后端。

1.13 版本之前：

- **MemoryStateBackend**：开发时使用。这是一种基于内存的状态后端，用于在开发过程中快速调试和测试 Flink 程序。由于它使用内存存储状态数据，因此适用于状态数据较小的场景。
- **FsStateBackend**：生产时使用，常用。这是一种基于文件系统的状态后端，将状态数据存储在磁盘上。FsStateBackend 提供了一种高可用性的状态备份和恢复机制，以确保在任务失败时能够恢复状态。
- **RocksDBStateBackend**：生产时使用，非常大的状态时用。这是一种基于 RocksDB 的状态后端，使用 RocksDB 数据库存储状态数据。RocksDB 是一种支持高效压缩和快速查找的键值存储系统，适用于处理大规模状态数据的场景。

1.13 版本之后：

- **HashMapStateBackend**：即 MemoryStateBackend 和 FsStateBackend，根据 API 不同。从 1.13 版本开始，Flink 对状态后端进行了整合，将 MemoryStateBackend 和 FsStateBackend 合并为一个统一的 HashMapStateBackend。它使用 HashMap 数据结构存储状态数据，并提供了一些额外的功能，如快照、checkpoint 等。
- **EmbeddedRocksDBStateBackend**：生产时使用，非常大的状态时用。这是一种基于 RocksDB 的状态后端，但与 RocksDBStateBackend 不同的是，它将 RocksDB 数据库嵌入到 Flink 的 TaskManager 进程中。这样做的优点是在状态数据较大时，可以减少网络开销和提高访问性能。
  总之，Flink 的状态储存系统包括多种状态后端，以适应不同的应用场景和需求。开发者可以根据实际情况选择合适的状态后端，以实现高效、可靠的 Flink 程序。

### 介绍一下Flink的CEP机制

Flink 的 CEP（Complex Event Processing，复杂事件处理）机制主要用于处理实时数据流中的复杂事件，以便实时地计算和响应这些事件。与传统的批处理方式不同，CEP 机制可以处理实时数据流中的事件，并根据事件的复杂逻辑进行实时计算和响应。

Flink CEP 是在 Flink 中实现的复杂事件处理（CEP）库。CEP 允许在无休止的事件流中检测事件模式，让我们有机会掌握数据中重要的部分。一个或多个由简单事件构成的事件流通过一定的规则匹配，然后输出用户想得到的数据——满足规则的复杂事件。

Flink 的 CEP 机制主要依赖于两个核心组件：Flink 的流处理框架和 CEP 库。Flink 的流处理框架提供了低延迟、高吞吐量的数据流处理能力，可以处理海量的实时数据。而 CEP 库则提供了处理复杂事件的逻辑，可以实现事件的过滤、聚合、路由等功能。通过这两个组件的结合，Flink 能够实现对实时数据流中复杂事件的实时处理和响应。

Flink 的 CEP 机制具有以下特点：

1. **实时性**：Flink 的 CEP 机制可以处理实时数据流中的事件，并实时计算和响应这些事件，具有非常低的延迟。
2. **灵活性**：CEP 库提供了灵活的事件处理逻辑，可以根据具体的业务需求定义事件的处理方式，例如：过滤、聚合、路由等。
3. **可扩展性**：Flink 的流处理框架具有优秀的水平扩展能力，可以根据数据流的规模和处理需求动态地增加或减少计算资源。
4. **高可用性**：Flink 的 CEP 机制支持故障恢复，可以在应用程序出现故障时自动恢复，避免数据丢失和影响。
5. **流式处理**：Flink 的 CEP 机制采用流式处理的方式处理实时数据流，可以实时地计算和响应事件，不需要先收集所有数据再进行批处理。

Flink 的 CEP 机制在实际应用中可以广泛应用于金融、物联网、物流等行业，例如：实时计算股票交易数据、实时监测传感器数据、实时路由物流信息等。了解 Flink 的 CEP 机制有助于更好地应对实时数据流中的复杂事件处理需求

### Flink的并行度是什么？Flink的并行度设置是怎么样的

Flink 的并行度是指在执行算子时，可以同时处理的数据流分片的数量。通过设置并行度，可以充分利用集群中的多个 TaskSlots（任务槽）来执行多个数据流分片，从而提高计算性能。并行度这个概念很好理解，例如 Kafka Source，它的并行度默认就是它的分区数量。

Flink 的并行度设置可以通过算子内部的参数或者外部的配置进行调整。

下面是一些常见的设置方法：

1. 对于内置算子，如 Map、Filter、Reduce 等，可以通过算子函数的参数来设置并行度。例如，在 Map 算子中，可以使用 `map_function.parallelism` 参数来设置并行度。
2. 对于自定义算子，可以通过实现 `ParallelismAware` 接口来设置并行度。在实现该接口的过程中，需要实现 `get_parallelism` 方法，该方法返回算子的并行度。
3. 在 Flink 的配置文件（如 `flink-config.yaml`）中，可以设置整个任务的并行度。例如，可以使用 `parallelism.task.num` 参数来设置 TaskSlots 的数量，从而影响算子的并行度。
   一般情况下，我们应该根据数据量来设置并行度。对于源算子（如 Kafka Source、HDFS Source 等），它们的并行度通常可以与分区数量保持一致，因为源算子通常不会产生太多的数据量。对于中间算子（如 Map、Filter 等），并行度可以根据数据量的大小进行适当调整。对于聚合算子（如 Reduce、Aggregate 等）和连接算子（如 Join 等），并行度通常需要根据数据量的大小和算子的压力来综合考虑

### 说说Flink的分区策略

Flink 提供了多种分区策略以满足不同数据处理的需求。以下是详细的叙述：

1. **GlobalPartitioner**：将数据发到下游算子的第一个实例。这种分区器适用于数据处理过程中只需要一个实例处理的情况。
2. **ShufflePartitioner**：将数据随机分发到下游算子。这种分区器适用于数据处理过程中需要对数据进行随机分发的情况，例如数据去重或数据混淆等。
3. **RebalancePartitioner**：将数据循环发送到下游的实例。这种分区器适用于数据处理过程中需要对数据进行循环处理的情况，例如数据清洗或数据转换等。
4. **RescalePartitioner**：根据上下游算子的并行度，循环输出到下游算子。这种分区器适用于数据处理过程中需要根据算子的并行度进行数据分配的情况，例如数据聚合或数据过滤等。
5. **BroadcastPartitioner**：输出到下游算子的每个实例中。这种分区器适用于数据处理过程中需要将数据广播到所有实例中的情况，例如数据源或数据收集等。
6. **ForwardPartitioner**：上下游算子并行度一样。这种分区器适用于数据处理过程中需要保持上下游算子的并行度一致的情况，例如数据窗口或数据排序等。
7. **KeyGroupStreamPartitioner**：按 Key 的 Hash 值输出到下游算子。这种分区器适用于数据处理过程中需要根据 Key 的哈希值进行数据分区的情况，例如数据分组或数据汇总等。
8. **KeyedStream**：根据 keyGroup 索引编号进行分区，会将数据按 Key 的 Hash 值输出到下游算子实例中。该分区器不是提供给用户来用的，而是 Flink 内部使用的。
9. **CustomPartitionerWrapper**：用户自定义分区器。这种分区器需要用户自己实现 Partitioner 接口，来定义自己的分区逻辑。适用于数据处理过程中需要根据特定逻辑进行数据分区的情况。

Flink 提供了多种内置的分区器以满足常见的数据处理需求，同时也支持用户自定义分区器以满足特定需求

### TaskSolt是什么

TaskSlot 是 Flink 中用于控制 TaskManager 接收任务的数量的一个概念。它是一个抽象的概念，表示一个 TaskManager 能够处理的任务数量。在 Flink 中，TaskManager 是实际执行程序的工作节点，为了起到资源隔离和并行执行的作用，TaskManager 是一个 JVM 进程。通过 TaskSlot 的概念，可以控制 TaskManager 接收的任务数量，从而更好地利用集群资源。

当有一个 source 需要指定三个并行度时，它就需要使用三个 TaskSlot。这是因为 TaskSlot 的数量决定了 TaskManager 能够处理的任务数量。如果 source 需要三个并行度，那么 TaskManager 就需要三个 TaskSlot 来处理这三个并行度的任务。

还有一个需要主要的优化概念是，当算子的并行度相同，并且没有发生并行度改变、或者没有 shuffle 时，这些算子会合并在一起。这样做的目的是为了减少资源的消耗，提高计算效率。

### Flink的Slots和并行度有什么关系

Solt 是 TaskManager 中的概念，表示 TaskManager 的一个槽（Slot）。并行度是程序中的概念，表示程序并行执行的程度。在 Flink 中，Solt 和并行度有着密切的关系。

具体来说，Solt 是 TaskManager 的资源分配单位，它决定了 TaskManager 能够支持的并行度。一个 TaskManager 有多个 Solt，每个 Solt 可以分配给一个 Task，用于执行程序。因此，TaskManager 的并行度就等于其 Solt 的数量。

程序制定的并行度使用的是槽（Solt），也就是说，程序是通过分配 Solt 来控制并行度的。当程序需要更高的并行度时，它可以向 TaskManager 申请更多的 Solt，以便在同一时间内执行更多的 Task。

因此，Solt 和并行度之间的关系可以总结为：Solt 是 TaskManager 中的概念，它决定了 TaskManager 能够支持的并行度；并行度是程序中的概念，它是通过分配 Solt 来控制的。TaskManager 是提供方，提供 Solt 资源给程序使用；程序是使用方，通过分配 Solt 来控制并行度。

### 说说Flink的资源调度

Flink 的资源调度是基于 TaskManager 和 Task slot 的概念进行的。TaskManager 是 Flink 中最小的调度单元，负责管理和调度任务。而 Task slot 则是 TaskManager 中最细粒度的资源，代表了一个固定大小的资源子集。每个 TaskManager 会将其所占有的资源平分给它的 slot。通过调整 task slot 的数量，用户可以定义 task 之间是如何相互隔离的。

每个 TaskManager 有一个 slot，也就意味着每个 task 运行在独立的 JVM 中。这样做的好处是，任务之间的隔离更加明确，一个任务出现问题不会影响到其他任务。同时，独立的 JVM 可以提供更好的资源管理和垃圾回收。

而当 TaskManager 拥有多个 slot 时，多个 task 可以运行在同一个 JVM 中。这样做的好处是，可以共享 TCP 连接（基于多路复用）和心跳消息，从而减少数据的网络传输。此外，同一个 JVM 进程中的 task 还可以共享一些数据结构，从而减少每个 task 的消耗。

在 Flink 中，每个 slot 可以接受单个 task，也可以接受多个连续 task 组成的 pipeline。例如，FlatMap 函数占用一个 taskslot，而 key Agg 函数和 sink 函数共用一个 taskslot。这种灵活的资源调度方式可以根据不同的任务需求进行优化和配置，提高系统的资源利用率和性能。

总之，Flink 的资源调度是通过 TaskManager 和 Task slot 的概念来实现的，通过调整 task slot 的数量和分配方式，可以满足不同任务的需求，提高系统的资源利用率和性能。

### Flink中有没有重启策略

Flink 中的重启策略用于在程序运行过程中发生故障时，如何重新启动算子以恢复程序的运行。重启策略可以在 flink-conf.yaml 中配置，也可以在应用代码中动态指定。

以下是 Flink 中常见的四种重启策略：

1. **故障延迟重启策略**（Failure Delay Restart Strategy）：当一个算子失败时，该策略会等待一个固定的时间间隔（即延迟时间）后，重新启动该算子。如果在延迟时间内，同一个算子再次失败，则会重新计算延迟时间，并将其设置为之前的两倍。这个过程会一直重复，直到延迟时间达到一个最大的值（通常是 60 秒），此时 Flink 会放弃重启该算子，并将其标记为永久失败。
2. **故障率重启策略**（Failure Rate Restart Strategy）：该策略基于算子的失败率来决定是否重新启动算子。当一个算子的失败率超过一个预设的阈值时，Flink 会重新启动该算子。这个策略适用于那些可能因为数据异常或程序 BUG 导致频繁失败的算子。
3. **没有重启策略**（No Restart Strategy）：当一个算子失败时，该策略不会重新启动该算子，而是直接跳过该算子，继续执行后面的算子。这个策略适用于那些可以在失败后被忽略的算子，例如那些只是输出数据的算子。
4. **Fallback 重启策略**（Fallback Restart Strategy）：当一个算子失败时，该策略会尝试重新启动该算子，如果重启失败，则会 fallback 到之前的版本，即不会重新启动该算子。这个策略适用于那些可能因为程序升级导致失败的算子，以便在重启失败时能够回滚到之前的版本。

如果没有启用 checkpointing，则使用无重启（no restart）策略。如果启用了 checkpointing，但没有配置重启策略，则使用固定间隔（fixed-delay）策略。在固定间隔策略中，Flink 会等待一个固定的时间间隔后，重新启动失败的算子。这个时间间隔可以通过 flink-conf.yaml 中的 restart.delay 配置项来设置。

### 如果下级存储不支持事务，Flink 怎么保证 exactly-once

端到端的 exactly-once 对 sink 要求比较高，具体实现主要有**幂等写入**和**事务性写入**两种方式。

幂等写入的场景依赖于业务逻辑，更常见的是用事务性写入。而事务性写入又有预写日志（WAL）和两阶段提交（2PC）两种方式。

如果外部系统不支持事务，那么可以用预写日志的方式，把结果数据先当成状态保存，然后在收到 checkpoint 完成的通知时，一次性写入 sink 系统。

### 对于迟到数据是怎么处理的

Flink中 WaterMark 和 Window 机制解决了流式数据的乱序问题，对于因为延迟而顺序有误的数据，可以根据eventTime进行业务处理，对于延迟的数据Flink也有自己的解决办法，主要的办法是给定一个允许延迟的时间，在该时间范围内仍可以接受处理延迟数据：

- 设置允许延迟的时间是通过allowedLateness(lateness: Time)设置
- 保存延迟数据则是通过sideOutputLateData(outputTag: OutputTag[T])保存
- 获取延迟数据是通过DataStream.getSideOutput(tag: OutputTag[X])获取



